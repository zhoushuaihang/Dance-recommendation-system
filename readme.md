# 舞蹈视频推荐系统：
主要分为三个实验阶段，数据构建以及处理；召回阶段；排序阶段


## 数据构建

####开发环境为：Windows10 + python3.8
本教程主要是针对B站舞蹈视频进行数据获取，数据来源于B站舞蹈区的中国舞和舞蹈教程两个板块，获取的视频日期为2021.08.01-2.21.12.31。主要分为五部分数据获取，每部分具体获取内容如下：

- **舞蹈信息表**，这部分按月份进行获取，由于舞蹈教程较少，因此月份合并获取。最后将所有获取的数据整合成一张表。
- **舞蹈_用户交互表**，这部分根据舞蹈信息表获取的视频url，获取评论区用户ID以及相关链接，并将label值设为1;为提高获取速度,将源表进行拆分,分为多张表在不同的代码以及IP环境中运行。
- **用户信息表**，首先根据交互表的user_id获取user_id的集合，然后根据user_id获取用户的公开信息。
- **舞蹈封面**，根据舞蹈信息表的pic的url获取封面图的二进制文件。
- **舞蹈视频**，首先根据舞蹈信息表的视频url获取视频以及音频的二进制文件，然后使用系统工具执行ffmpeg命令，进行音视频合并。

####数据集获取技术总结：
	1.python的基本语法：基本语法元素、数据类型（整数、浮点数、字符串）、分支结构（判断、循环）、异常处理、time库、random库、函数、组合数据类型（集合、列表、字典），文件和数据格式化（打开、读取、写入）、枚举函数
	2.各类处理文件库（json、csv、pysql、xlrd、xlwt、openpyxl）
    3.网页请求库（request），正则解析库（re）
    4.数据分析库（numpy、pandas）
    5.ffmpeg（多媒体支持库）：此次只用到视频和音频的合成

6. subprocess模块来管理子进程、os库负责操作系统交互功能



## 召回阶段

为处理在复杂业务场景和大数据场景下的推荐问题，目前主流的工业级推荐业务流程一般划分为召回层以
及排序层两个阶段。召回算法是将用户可能感兴趣的物品从全量数据集中取出，由于物品候选库数量巨大，
使用的召回模型特点是简单高效，从而可以实现快速召回；排序层负责将召回的候选集按照用户可能点击
的概率进行精准排序，为提高排序的准确性，往往采用较为复杂的深度学习模型充分挖掘用户潜在的更深层次的
关系。两个阶段组合成推荐系统的数据流架构。


- **特征工程**，由于舞蹈视频相关数据数据经过初步的数据清洗后并不具有良好的特征表达，极大降低了机器学习模型对数据的拟合的效果，所以需要对数据进行特征工程以找到良好的特征表达。该数据集中存在的大量离散值特征，如用户 ID、舞蹈ID、舞蹈标题、舞蹈标签等等，将这些特征转换成特征向量，对非文本的离散值特征使用词嵌入方法,对文本特征用Word2vec模型进行无监督的学习训练,并且根据训练好的模型获得具语义相关性的词向量。

- **item_cf**，基于物品的协同过滤（item-based collaborative filtering）算法是最经典的推荐算法之一，同时也是业界应用最多的算法。该算法核心思想是：物品A和物品B具有很大的相似度的原因是喜欢物品A的用户大概率也喜欢物品B。
- **FM**，由于数据之间存在辛普森悖论，即在某个条件下的两组数据，分开讨论分析时会同时满足某种性质，而一旦合并属性，有可能得到相反的结论。为充分利用特征之间的线性关系，让模型学习到二阶交互特征。
- **DSSM**，该模型通过双塔深度神经网络将用户查询和文本分离地映射到同一隐语义空间中，进而实现用户查询和文本内容的表征与匹配。凭借其出色的隐语义提取与表征能力，DSSM被广泛地应用于推荐召回阶段实现用户与物品之间的匹配，其解决思路是分别从用户侧和物品侧构建对应的隐语义特征表示向量，并基于用户和物品的隐语义特征表示向量，通过余弦相似度计算得到候选物品的相似度排序结果，以获得个性化推荐召回候选物品子集。
- **YoutobeDNN**，YoutubeDNN是Youtube用于做视频推荐的落地模型，可谓推荐系统中的经典，其大体思路为召回阶段使用多个简单模型筛除大量相关度较低的样本，排序阶段使用较为复杂的模型获取精准的推荐结果。借用YoutobeDNN的网络模型，搭建适合该数据场景下的模型也取得不错的效果。



## 排序阶段

本质是一个端到端的模型，尽可能的利用大量的特征以及更复杂的深度学习模型来挖掘更深层次的信息，通过深度学习的各个模块使得输入的用户特征、物品特征以及上下文特征充分交叉，从而对召回层获取的候选集进行海量且复杂的迭代运算来提高用户点击的准确度。


- **Bert编码**，BERT（Bidirectional Encoder Representations from Transformers）是一种Transformer的双向编码器，
  旨在通过在左右上下文中共有的条件计算来预先训练来自无标号文本的深度双向表示。
  因此，经过预先训练的BERT模型只需一个额外的输出层就可以进行微调，从而为各种自然语言处理任务生成最新模型，也可以利用预训练好的Bert模型进行文本特征词向量或者句向量提取。
  由于舞蹈视频相关数据数据经过初步的数据清洗后并不具有良好的特征表达，
  极大降低了机器学习模型对数据的拟合的效果，所以需要对数据进行特征工程以找到良好的特征表达
  。该数据集中存在的大量离散值特征，如用户 ID、舞蹈ID、舞蹈标题、舞蹈标签等等，将这些特征转换成特征向量，
  对非文本的离散值特征使用词嵌入方法,对文本特征用使用BERT进行文本特征提取,从而获得具语义相关性的句向量。


- **DNN**，深度神经网络（Deep Neural Networks， DNN），也称深度前馈网络（Deep Feedfoward Network，DFN）、
  前馈神经网络（Feedforward Neural Network，FNN）、多层感知机（multilayer perceptron，MLP）。通过输入层输入用户以及舞蹈特征计算点击率值。DNN结构很灵活，自由度高，经实验得出结果优于传统机器学习算法。但相对经典的深度学习推荐模型，效果差距过大。




- **Wide&Deep**，推荐系统和搜索排序共有的一大挑战为同时具备记忆性和泛化性（同时获得推荐结果准确性和扩展性）。
  记忆能力可以解释为学习那些经常同时出现的特征，发掘历史数据中存在的共现性。泛化能力则基于迁移相关性，探索之前几乎没有出现过的新特征组合。
  基于记忆能力的推荐系统通常偏向学习历史数据的样本，直接与用户的历史数据相关;泛化能力相比记忆能力则更趋向于提高推荐结果的丰富性。




- **DeepFM**，结合因子分解机与深度神经网络在特征学习中的优点：同时提取到低阶组合特征与高阶组合特征，所以越来越被广泛使用。
  DeepFM中，FM算法负责对一阶特征以及由一阶特征两两组合而成的二阶特征进行特征的提取；DNN算法负责对由输入的一阶特征进行全连接等操作形成的高阶特征进行特征的提取。
  结合广度和深度模型的优点，联合训练FM模型和DNN模型，同时学习低阶特征组合和高阶特征组合。端到端模型，无需特征工程。




- **Deep & Cross**，使用cross network，在每一层都应用feature crossing。高效的学习了bounded degree组合特征。不需要人工特征工程。
网络结构简单且高效。多项式复杂度由layer depth决定。
相比于DNN，DCN的logloss更低，而且参数的数量将近少了一个数量级。


- **AutoInt**，引入了transformer的核心模块多头的注意力机制(multi-head self-attention)来实现特征之间的高阶显性交互， 根据不同的相关性策略去让特征交互然后融合，在这个交互过程中，特征之间计算相关性得到权重，并加权汇总，使得最终每个特征上都有了其它特征的信息，且其它特征的信息重要性还有了权重标识。 这个过程的自注意力计算以及汇总是一个自动的过程，并且模型结构清晰。



- **xDeepFM**，提出压缩交互网络(Compressed Interaction Network(CIN))，能够学习显式的交互特征并且是在vector-wise的级别，CIN带有一些CNN和RNN的特点，具有更高的泛化能力(vector-wise)并且实现特征显式高阶自动交叉。



- **FiBiNET**，引入一个SENet模块，可以动态学习特征的重要性；引入一个双线性模块（Bilinear-Interaction layer），增强二阶特征交互。


#####在排序阶段采用word2vec提取的特征向量以及Bert提取的特征向量进行对比试验，其中Bert提取向量训练的结果要明显优于word2vec提取特征向量训练结果。